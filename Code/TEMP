
from together import Together
import datetime as dt


TogetherAPIKey = "9b063679d208383fe3590644d40cc4ba90ddf213af5772395d80d570087ba84a"
NewsAPIKey = "6a8bb318a95b4a6784f3bedf6ca5e370"
newsurl = "https://newsapi.org/v2/top-headlines"

import os
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain_community.utilities.duckduckgo_search import TavilySearchAPIWrapper
from langchain.llms.base import LLM
from typing import Optional, List, Mapping, Any
import together


class TogetherLLM(LLM):
    model: str = "meta-llama/Llama-3-70b-chat-hf"
    temperature: float = 0.7
    max_tokens: int = 3
    together_api_key: Optional[str] = TogetherAPIKey

    @property
    def _llm_type(self) -> str:
        return "together-custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        together.api_key = self.together_api_key
        response = together.Complete.create(
            prompt=prompt,
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stop=stop,
            together_api_key= TogetherAPIKey
        )
        return response["choices"][0]["text"]
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
# ChatBot.py

# Initialize Together AI LLM
llm = TogetherLLM()

# Web search tool (Tavily)
search = TavilySearchAPIWrapper(tavily_api_key="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")

tools = [
    Tool(
        name="Web Search",
        func=search.results,
        description="Useful for answering questions about current events from the internet."
    )
]

# Initialize the agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Run the agent
while True:
    query = input("You: ")
    if query.lower() in ["exit", "quit"]:
        break
    result = agent.run(query)
    print("\nðŸ¤– Agent:", result)



# botMAIN
import ChatBot as chatBot
import BrokerBot as brokerBot
import NewsBot as newsBot
import requests
from bs4 import BeautifulSoup
from sec_edgar_downloader import Downloader
import re
import feedparser 
from html import unescape
import os
import csv
from dataclasses import dataclass
import datetime as dt
Watchlist = {}




ticker = "IXHL"  
Cik = ""

rss_url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={Cik}&count=3&output=atom"


@dataclass
class PotentialBoom:
    ticker: str
    cik: str
    name: str
    event: str
    date_of_catalyst: str

def load_catalysts(filepath: str):
    booms = []
    with open(filepath, newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            booms.append(PotentialBoom(**row))
    return booms

def parse_sec_submission(file_path):
    WANTED_TYPES = {
        "8-K", "EX-99.1", "EX-99.2", "S-1", "10-K", "10-Q", "DEF 14A", "S-3", "S-4", "424B5", "424B3", "SD", "SC 13G", "SC 13D", "PRE 14A", "DEFA14A", "EX-10.1", "EX-10.2", "EX-23", "EX-21", "EX-31", "EX-32"
    }  # Add/remove as needed for your use case

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    documents = re.findall(r"<DOCUMENT>(.*?)</DOCUMENT>", content, re.DOTALL)
    parsed_docs = []

    for doc in documents:
        doc_type_match = re.search(r"<TYPE>(.*)", doc)
        description_match = re.search(r"<DESCRIPTION>(.*)", doc)
        text_match = re.search(r"<TEXT>(.*)", doc, re.DOTALL)

        doc_type = doc_type_match.group(1).strip() if doc_type_match else ""
        if doc_type not in WANTED_TYPES:
            continue  # Skip unwanted sections

        # Extract and clean the text content
        def remove_tags(text):
            return re.sub(r"<.*?>", "", text, flags=re.DOTALL)

        raw_text = text_match.group(1).strip() if text_match else ""
        raw_text = remove_tags(raw_text)


        patterns = [
            r"(?i)forward[-\s]?looking statements.*?(?=ITEM\s+\d+|SIGNATURES|\Z)",
            r"(?i)safe harbor.*?(?=ITEM\s+\d+|SIGNATURES|\Z)",
            r"(?i)risks related to.*?(?=ITEM\s+\d+|SIGNATURES|\Z)",
            r"(?i)cautionary note.*?(?=ITEM\s+\d+|SIGNATURES|\Z)",
        ]

        for p in patterns:
            raw_text = re.sub(p, "", raw_text, flags=re.DOTALL)

        clean_text = unescape(raw_text)
        # If 8-K, remove everything after "Item 9.01 Financial Statements and Exhibits.\n(d) Exhibits"
       
        clean_text = "\n".join(line.strip() for line in clean_text.splitlines() if line.strip())
        # Remove everything before "standards provided pursuant to Section 13(a) of the Exchange Act."
        marker = "of the Exchange Act."
        idx = clean_text.find(marker)
        if idx != -1:
            clean_text = clean_text[idx:]
        
        if doc_type == "8-K":
            end_marker = "The information contained in this Current Report shall not be deemed"
            end_idx = clean_text.find(end_marker)
            if end_idx != -1:
                clean_text = clean_text[:end_idx]
            end_idx = clean_text.find(end_marker)
            if end_idx != -1:
                clean_text = clean_text[:end_idx + len(end_marker)]
        # Unescape HTML entities and remove excessive blank lines/whitespace
    

        parsed_docs.append({
            "type": doc_type,
            "description": description_match.group(1).strip() if description_match else "",
            "text": clean_text
        })

    return parsed_docs

# Helper function to get CIK for a ticker
def get_cik_for_ticker(ticker):

    dl = Downloader("JustAChillGuy", "JustAChillGuy@gmail.com")
    return dl.ticker_to_cik_mapping[ticker]

# gets  new filings for a given CIK and last known date
def get_new_forms(last_known_date):
    url = f"https://data.sec.gov/submissions/CIK{Cik}.json"
    headers = {"User-Agent": "YourName your@email.com"}
    resp = requests.get(url, headers=headers)
    data = resp.json()
    recent = data.get("filings", {}).get("recent", {})
    forms = recent.get("form", [])
    filing_dates = recent.get("filingDate", [])
    found_dates = []
    found_forms = []
    for form, date in zip(forms[:3], filing_dates[:3]):
        if date > last_known_date:
            found_dates.append(date)
            found_forms.append(form)
    if found_dates:
        return True, found_forms, found_dates
    return False, None, None

def get_last_known_date():
    
    url = f"https://data.sec.gov/submissions/CIK{Cik}.json"
    headers = {"User-Agent": "YourName your@email.com"}
    resp = requests.get(url, headers=headers)
    data = resp.json()
    recent = data.get("filings", {}).get("recent", {})
    filing_dates = recent.get("filingDate", [])
    return filing_dates[0] if filing_dates else None

def main(forms):

    dl = Downloader("JustAChillGuy", "JustAChillGuy@gmail.com")
    for form in forms:
        dl.get(form, ticker, limit=3, download_details=False)
    
    base_dir = dl.download_folder
    docs = []
    TextWall = ""

    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".txt"):
                file_path = os.path.join(root, file)
                docs.extend(parse_sec_submission(file_path))
    for doc in docs:
        TextWall += ((f"File Type: {doc['type']}\nQuick Description: {doc['description']}\nText: {doc['text'][:30000]}\n{'-'*40}"))
    print(TextWall)

def Initialize():
    brokerBot.Start()

def WaitingLoop():
    while True:
        someBool, forms, dates = get_new_forms(last_known_date)  # Use a very old date to ensure we get the latest filings
        if someBool:
            print(f"New filing(s) found for {ticker} on {dates} of type(s) {forms}.")  
            main(forms)
        else:
            print(f"No new filings for {ticker} since {last_known_date}.")

        brokerBot.ib.sleep(2)



Cik = get_cik_for_ticker(ticker)
last_known_date = get_last_known_date()

#WaitingLoop()
print(load_catalysts("Catalysts.csv"))
date = dt.date.today()
#main()